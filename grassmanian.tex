
\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Giving Words some Space: Embedding Words as $n$-Dimensional Subspaces}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Souvik Banerjee, Siddharth Bhat \& Alok Debnath \\ %\thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Language Technologies Research Center (LTRC)\\
International Institute of Information Technology, Hyderabad\\
Hyderabad, India \\
\texttt{\{souvik.banerjee, siddharth.bhat, alok.debnath\}@research.iiit.ac.in} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
    Embedding words in vector spaces has become the norm for distributional lexical semantic representations. While esoteric literature has attempted to change the underlying embedding space, the resultant word embeddings are specific to a given task or linguistic property. In this paper, we hypothesize that word representations need not change the embedding space, rather the object they are embedded as. We address the notion of a context vector introduced in the skip-gram, CBOW and GloVe models, and test our hypothesis by analyzing word embeddings in the symplectic manifold, where each word is represented by a 2-dimensional pair of (position, momentum), as proposed by Hamiltonian mechanics. We further generalize our representation to embedding words in a Grassmanian space $\mathbf{Gr}(p, V)$, where each word is a $p$-dimensional subspace of a larger $n$ dimensional vector space $V$, and similarity and analogy are computed exploiting the Lie theoretic structure of the Grassmanian; its exponential map, retraction, and parallel transport of geodesics. Finally, we provide rigorous empirical evidence and theoretical insight into our embeddings' ability to capture sense information and latent syntactic characteristics, improving the expressivity of the overall representation.
\end{abstract}

\section{Introduction}
\label{sec: intro}

Word representation is a central challenge in natural language processing. The terms \emph{word vectors} and \emph{word embeddings} synonymous to one another, due to the success of vector based models such as word2vec, GloVe and FastText. Even with the introduction of contextualized word representations, the underlying structure of word representations has remained the same, mapping a word to a single vector in a continuous vector space. Some of the known challenges with this assumption of word to vector have been based around resolving polysemy and the treatment of function words, to which the classical answer has been extracting and using contextual information. 

% Esoteric literature has motivated the modification of the vector space to a different mathematical object, either for the sake of task specific interpretability, or for enforcing the encoding of linguistic information based on human-created resources. However, these representations suffer from issues in scalability and generalization, therefore the interpretability afforded by these embeddings does not transport when replaced by a different task, or the need for a specific natural language ontologies, which are time and effort-intensive to create and use.

\section{Preliminaries}

\subsection{Words, Vectors and Expressivity}

\subsection{Lexical Semantics on a Symplectic Manifold}

\subsection{The Grassmanian Space: Generalizing to $n$-Dimensions}

\section{Training Mechanism}

\section{Results and Evaluation}

\section{Analysis}

\section{Related Work}

\section{Conclusion}

\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\appendix
\section{Appendix}

\end{document}

